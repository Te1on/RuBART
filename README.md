# RuBART
Модель [BART](https://research.fb.com/wp-content/uploads/2020/06/BART-Denoising-Sequence-to-Sequence-Pre-training-for-Natural-Language-Generation-Translation-and-Comprehension.pdf) для с **русского языка**.

## Описание

Задача данного проекта состоит в том чтобы обучить модель генерировать краткую аннотацию новостей на русском языке.
Почему BART? Затем что для его обучения не нужно колоссальных вычислительных мощностей, к тому же показывает неплохие результаты с меньшим количеством параметров.

Зачем? В образовательных целях.

Шаги для выполнения данной задачи:
- [x] 1. Прочитать и осмыслить статью авторов модели.
- [x] 2. Обучить BPE токенизатор на русском.
- [ ] 3. Собрать данные для предобучения модели.
- [ ] 4. Разобраться с AWS и предобучить модель.
- [ ] 5. Дообучить модель.

## 1. Прочитать и осмыслить статью авторов модели.

> BART is sequence-to-sequence model trained with denoising as pretraining objective. 
https://www.machinecurve.com/index.php/question/what-is-the-bart-transformer-in-nlp/

Архитектура большой модели: 12 слоев **Encoder** как в BERT'е и 12 слоев **Decoder** как в GPT-2, с соотвествующими attention mask. У BERT'а все слова в предложении полностью видимы для каждого слова, в GPT-2 только предыдущие слова.

К тому же при предобучении авторы предлагают задачу заполнения масок с зашумленным данными.

### Способы зашумления данных

- Маскирование токенов.
- Удаление токенов. Таким образом модель должна определить где находились удаленные токены.
- Текстовое заполнение. Маскирование одним токеном [MASK] кусков текста взятой длины из распределения Пуассона, если длина 0 то это соответствует вставке токена [MASK].
- Перемешка предложений. Документы с несколькими предложениями разделенные точкой перетасовываются в случайном порядке.
- Поворот документа. Токен выбирается равномерно случайным образом, и документ поворачивается так, чтобы он начинался с этого токена. Эта задача обучает модель определять начало документа.

После экспериментов авторы решили скомбинировать маскирование 30% токенов и перемешивание всех предложений.

### Что в итоге?

Путем комбинации преимуществ архитектуры моделей BERT и GPT собрали модель Seq2Seq предобученную на зашумленных данных, что позволило ей достичь state-of-the-art в некоторых задачах генерации текста.

## 2. Обучить BPE токенизатор на русском.

В качестве обучающих данных был взят датасет [News dataset from Lenta.Ru](https://www.kaggle.com/yutkin/corpus-of-russian-news-articles-from-lenta).

## 3. Собрать данные для предобучения модели.

## 4. Разобраться с AWS и предобучить модель.

## 5. Дообучить модель.
